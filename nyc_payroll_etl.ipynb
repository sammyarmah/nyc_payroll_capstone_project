{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyarrow) (1.23.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (12.22.0)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (1.31.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (43.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (4.12.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core>=1.28.0->azure-storage-blob) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary packages\n",
    "!pip install pyarrow\n",
    "!pip install azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import os \n",
    "import io\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of raw data files\n",
    "agency_df = pd.read_csv(r'raw_files\\AgencyMaster.csv')\n",
    "employemnt_df = pd.read_csv(r'raw_files\\EmpMaster.csv')\n",
    "payroll_2020_df = pd.read_csv(r'raw_files\\nycpayroll_2020.csv')\n",
    "payroll_2021_df = pd.read_csv(r'raw_files\\nycpayroll_2021.csv')\n",
    "title_df = pd.read_csv(r'raw_files\\TitleMaster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "title_df.fillna({\n",
    "    'TitleDescription' : 'unknown'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tables\n",
    "agency_dim = agency_df[['AgencyID', 'AgencyName']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "agency_dim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee table\n",
    "employee_dim = employemnt_df[['EmployeeID', 'LastName', 'FirstName']].copy().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title table\n",
    "title_dim = title_df[['TitleCode', 'TitleDescription']].copy().drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the payroll data for 2020 and 2021\n",
    "payroll_combined = pd.concat([payroll_2020_df, payroll_2021_df], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame to confirm\n",
    "payroll_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled payroll_combined with missing values\n",
    "payroll_combined.fillna({\n",
    "    'AgencyID' : 0,\n",
    "    'AgencyCode': 0\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing datatypes to integer\n",
    "payroll_combined['AgencyID'] = payroll_combined['AgencyID'].astype(int)\n",
    "payroll_combined['AgencyCode'] = payroll_combined['AgencyCode'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeID values in payroll_combined.csv have been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming EmployeeID is the column name and needs to be converted to 6 digits by adding leading zeros\n",
    "\n",
    "payroll_combined['EmployeeID'] = payroll_combined['EmployeeID'].apply(lambda x: int(str(x).zfill(6)))\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "#payroll_combined.to_csv(payroll_combined_path, index=False)\n",
    "\n",
    "print(\"EmployeeID values in payroll_combined.csv have been updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "payroll_facts_table = payroll_combined.merge(employee_dim, on= ['EmployeeID', 'LastName', 'FirstName'], how = 'left') \\\n",
    "                                     .merge(title_dim, on = ['TitleCode', 'TitleDescription'], how = 'left') \\\n",
    "                                     .merge(agency_dim, on = ['AgencyID', 'AgencyName'], how = 'left') \\\n",
    "                                     [['PayrollNumber', 'EmployeeID', 'AgencyID','TitleCode', 'FiscalYear', 'AgencyStartDate', 'WorkLocationBorough', 'AgencyCode', \\\n",
    "                                        'LeaveStatusasofJune30', 'BaseSalary','PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files into csv\n",
    "employee_dim.to_csv(r'dataset\\employee_dim.csv', index=False)\n",
    "title_dim.to_csv(r'dataset\\title_dim.csv', index=False)\n",
    "agency_dim.to_csv(r'dataset\\agency_dim.csv', index=False)\n",
    "payroll_facts_table.to_csv(r'dataset\\payroll_facts_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the Azure blob connection and load data\n",
    "load_dotenv()\n",
    "\n",
    "connect_str = os.getenv('CONNECT_STR')\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "container_name = os.getenv('CONTAINER_NAME')\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into Azure blob storage as a parquet file\n",
    "def upload_df_to_blob_as_parquet(df, container_client,blob_name):\n",
    "    buffer = io.BytesIO()\n",
    "    df.to_parquet(buffer, index=False)\n",
    "    buffer.seek(0)\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    blob_client.upload_blob(buffer, blob_type=\"BlockBlob\", overwrite=True)\n",
    "    print(f'{blob_name} uploaded to Blob storage successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawdata/agency_dim.parquet uploaded to Blob storage successfully\n",
      "rawdata/employee_dim.parquet uploaded to Blob storage successfully\n",
      "rawdata/payroll_facts_table.parquet uploaded to Blob storage successfully\n",
      "rawdata/title_dim.parquet uploaded to Blob storage successfully\n"
     ]
    }
   ],
   "source": [
    "upload_df_to_blob_as_parquet(agency_dim, container_client, 'rawdata/agency_dim.parquet')\n",
    "upload_df_to_blob_as_parquet(employee_dim, container_client, 'rawdata/employee_dim.parquet')\n",
    "upload_df_to_blob_as_parquet(payroll_facts_table, container_client, 'rawdata/payroll_facts_table.parquet')\n",
    "upload_df_to_blob_as_parquet(title_dim, container_client, 'rawdata/title_dim.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into RDBMS using psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\armah_samuel\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary package\n",
    "import psycopg2\n",
    "\n",
    "# Loading data to postgresql\n",
    "def get_db_connection():\n",
    "    conn = psycopg2.connect(\n",
    "        host = 'localhost',\n",
    "        database = 'nyc_payroll',\n",
    "        user = 'postgres',\n",
    "        password = 'genesis23NeW'   \n",
    "    )\n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create tables\n",
    "def create_table():\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    create_table_query = '''\n",
    "                         -- Drop tables if they exist\n",
    "                         DROP TABLE IF EXISTS payroll_facts_table;\n",
    "                         DROP TABLE IF EXISTS agency_dim;\n",
    "                         DROP TABLE IF EXISTS employee_dim;\n",
    "                         DROP TABLE IF EXISTS title_dim;\n",
    "\n",
    "                         -- Create agency_dim table\n",
    "                         CREATE TABLE IF NOT EXISTS agency_dim (\n",
    "                             AgencyID INT PRIMARY KEY,\n",
    "                             AgencyName VARCHAR(255)  -- Using a more reasonable length\n",
    "                         );\n",
    "\n",
    "                         -- Create employee_dim table\n",
    "                         CREATE TABLE IF NOT EXISTS employee_dim (\n",
    "                             EmployeeID INT PRIMARY KEY,\n",
    "                             LastName VARCHAR(255),\n",
    "                             FirstName VARCHAR(255)\n",
    "                         );\n",
    "\n",
    "                         -- Create title_dim table\n",
    "                         CREATE TABLE IF NOT EXISTS title_dim (\n",
    "                             TitleCode INT PRIMARY KEY,\n",
    "                             TitleDescription VARCHAR(255)\n",
    "                         );\n",
    "\n",
    "                         CREATE TABLE IF NOT EXISTS payroll_facts_table(\n",
    "                             PayrollNumber INT PRIMARY KEY,\n",
    "                             EmployeeID INT,\n",
    "                             AgencyID INT,\n",
    "                             TitleCode INT,\n",
    "                             FiscalYear INT,\n",
    "                             AgencyStartDate VARCHAR(100),\n",
    "                             WorkLocationBorough VARCHAR(255),\n",
    "                             AgencyCode INT,\n",
    "                             LeaveStatusasofJune30 VARCHAR(255),\n",
    "                             BaseSalary FLOAT,\n",
    "                             PayBasis VARCHAR(255),\n",
    "                             RegularHours FLOAT,\n",
    "                             RegularGrossPaid FLOAT,\n",
    "                             OTHours FLOAT,\n",
    "                             TotalOTPaid FLOAT,\n",
    "                             TotalOtherPay FLOAT,\n",
    "                             FOREIGN KEY (EmployeeID) REFERENCES employee_dim(EmployeeID),\n",
    "                             FOREIGN KEY (AgencyID) REFERENCES agency_dim (AgencyID),\n",
    "                             FOREIGN KEY (TitleCode) REFERENCES title_dim(TitleCode)\n",
    "                         );  \n",
    "                         '''\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agency_dim data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Loading the agency_dim.csv data\n",
    "import csv\n",
    "def load_data_from_csv(csv_path):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            cursor.execute(\n",
    "                '''INSERT INTO agency_dim(AgencyID, AgencyName)\n",
    "                   VALUES (%s, %s);''',\n",
    "                   row\n",
    "            )\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "# csv file path to the file\n",
    "csv_file_path = r'dataset\\agency_dim.csv'\n",
    "\n",
    "load_data_from_csv(csv_file_path)\n",
    "print('agency_dim data loaded successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employee_dim data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# loading employee_dim.csv data\n",
    "def load_data_from_csv(csv_path):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            cursor.execute(\n",
    "                '''INSERT INTO employee_dim(EmployeeID, LastName, FirstName)\n",
    "                   VALUES (%s, %s, %s);''',\n",
    "                   row\n",
    "            )\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "# csv file path to the file\n",
    "csv_file_path = r'dataset\\employee_dim.csv'\n",
    "\n",
    "load_data_from_csv(csv_file_path)\n",
    "print('employee_dim data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_dim data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# loading employee_dim.csv data\n",
    "def load_data_from_csv(csv_path):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            cursor.execute(\n",
    "                '''INSERT INTO title_dim(TitleCode, TitleDescription)\n",
    "                   VALUES (%s, %s);''',\n",
    "                   row\n",
    "            )\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "# csv file path to the file\n",
    "csv_file_path = r'dataset\\title_dim.csv'\n",
    "\n",
    "load_data_from_csv(csv_file_path)\n",
    "print('title_dim data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading payroll_facts_table.csv data\n",
    "def load_data_from_csv(csv_path):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                row[0] = int(float(row[0]))  # PayrollNumber\n",
    "                row[1] = int(float(row[1]))  # EmployeeID\n",
    "                row[2] = int(float(row[2]))  # AgencyID\n",
    "                row[3] = int(float(row[3]))  # TitleCode\n",
    "                row[4] = int(float(row[4]))  # FiscalYear\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting row: {row}\")\n",
    "                print(e)\n",
    "                continue \n",
    "            \n",
    "            \n",
    "            cursor.execute(\n",
    "                '''INSERT INTO payroll_facts_table(PayrollNumber, EmployeeID, AgencyID, TitleCode, FiscalYear, \\\n",
    "                    AgencyStartDate, WorkLocationBorough, AgencyCode, LeaveStatusasofJune30, BaseSalary, \\\n",
    "                        PayBasis, RegularHours, RegularGrossPaid, OTHours, TotalOTPaid, TotalOtherPay)\n",
    "                   VALUES (%s, %s, %s, %s, %s, %s,%s, %s, %s,%s, %s, %s,%s, %s, %s, %s)\n",
    "                   ON CONFLICT (PayrollNumber) DO NOTHING;''',\n",
    "                   tuple(row)\n",
    "            )\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "# csv file path to the file\n",
    "csv_file_path = r'dataset\\payroll_facts_table.csv'\n",
    "\n",
    "load_data_from_csv(csv_file_path)\n",
    "print('payroll_facts_table data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
